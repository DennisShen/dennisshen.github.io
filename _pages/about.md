---
permalink: /
title: "About Me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

I am a Ph.D. candidate in Electrical and Computer Engineering at the [University of Maryland, College Park](https://ece.umd.edu/), working with [Prof. Shuvra S. Bhattacharyya](https://user.eng.umd.edu/~ssb/) in the [Maryland DSPCAD Research Group](https://code.umd.edu/dspcad-pub/dspcadwiki/-/wikis/Maryland-DSPCAD-Research-Group). I received my M.S. degree from the [Graduate Institute of Electronics Engineering](https://giee.ntu.edu.tw/en/home/) at National Taiwan University, where I was advised by [Prof. Liang-Gee Chen](https://www.ee.ntu.edu.tw/profile1.php?id=26) in the [DSPIC Lab](https://homepage.ntu.edu.tw/~lgchen/index.html). I earned my B.S. degree in Electrical Engineering from [National Taiwan University](https://web.ee.ntu.edu.tw/eng/index.php).

Research
======
Like many other Jekyll-based GitHub Pages templates, Academic Pages makes you separate the website's content from its form. The content & metadata of your website are in structured markdown files, while various other files constitute the theme, specifying how to transform that content & metadata into HTML pages. You keep these various markdown (.md), YAML (.yml), HTML, and CSS files in a public GitHub repository. Each time you commit and push an update to the repository, the [GitHub pages](https://pages.github.com/) service creates static HTML pages based on these files, which are hosted on GitHub's servers free of charge.

Many of the features of dynamic content management systems (like Wordpress) can be achieved in this fashion, using a fraction of the computational resources and with far less vulnerability to hacking and DDoSing. You can also modify the theme to your heart's content without touching the content of your site. If you get to a point where you've broken something in Jekyll/HTML/CSS beyond repair, your markdown files describing your talks, publications, etc. are safe. You can rollback the changes or even delete the repository and start over - just be sure to save the markdown files! Finally, you can also write scripts that process the structured data on the site, such as [this one](https://github.com/academicpages/academicpages.github.io/blob/master/talkmap.ipynb) that analyzes metadata in pages about talks to display [a map of every location you've given a talk](https://academicpages.github.io/talkmap.html).

Preprints
======
<div style="display: flex; align-items: flex-start; margin-bottom: 20px; position: relative;">
  <img src="/images/autocompose.jpg" alt="AutoComPose" style="width: 150px; height: auto; margin-right: 20px; border-radius: 4px;" class="original-image">
  <div>
    <h3 style="margin: 0; font-size: 0.8em;">
      AutoComPose: Automatic Generation of Pose Transition Descriptions for Composed Pose Retrieval Using Multimodal LLMs
    </h3>
    <p style="margin: 5px 0; font-size: 0.8em;">
      <strong>Yi-Ting Shen*</strong>, Sungmin Eum*, Doheon Lee, Rohit Shete, Chiao-Yi Wang, Heesung Kwon, and Shuvra S. Bhattacharyya (* equal contribution)
      <br>
      <a href="https://arxiv.org/abs/2503.22884">[arXiv]</a>
    </p>
    <p style="margin: 0; font-size: 0.8em;">
      We introduce AutoComPose, the first framework to automatically generate pose transition annotations using multimodal large language models, significantly improving composed pose retrieval performance while reducing reliance on costly human labeling.
    </p>
  </div>
  <div class="enlarged-image-container">
    <img src="/images/autocompose.jpg" alt="AutoComPose Enlarged" class="enlarged-image">
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 20px; position: relative;">
  <img src="/images/synplay.png" alt="SynPlay" style="width: 150px; height: auto; margin-right: 20px; border-radius: 4px;" class="original-image">
  <div>
    <h3 style="margin: 0; font-size: 0.8em;">
      SynPlay: Importing Real-world Diversity for a Synthetic Human Dataset
    </h3>
    <p style="margin: 5px 0; font-size: 0.8em;">
      Jinsub Yim, Hyungtae Lee, Sungmin Eum, <strong>Yi-Ting Shen</strong>, Yan Zhang, Heesung Kwon, and Shuvra S. Bhattacharyya
      <br>
      <a href="https://arxiv.org/abs/2408.11814">[arXiv]</a> <a href="https://synplaydataset.github.io/">[Project]</a>
    </p>
    <p style="margin: 0; font-size: 0.8em;">
      We present Synthetic Playground (SynPlay), a large-scale synthetic human dataset with diverse motions and camera viewpoints—especially aerial views—that significantly improves human identification performance in challenging, data-scarce scenarios like few-shot learning and cross-domain adaptation.
    </p>
  </div>
  <div class="enlarged-image-container">
    <img src="/images/synplay.png" alt="SynPlay Enlarged" class="enlarged-image">
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 20px; position: relative;">
  <img src="/images/synposediv.jpg" alt="SynPoseDiv" style="width: 150px; height: auto; margin-right: 20px; border-radius: 4px;" class="original-image">
  <div>
    <h3 style="margin: 0; font-size: 0.8em;">
      Diversifying Human Pose in Synthetic Data for Aerial-view Human Detection
    </h3>
    <p style="margin: 5px 0; font-size: 0.8em;">
      <strong>Yi-Ting Shen*</strong>, Hyungtae Lee*, Heesung Kwon, and Shuvra S. Bhattacharyya (* equal contribution)
      <br>
      <a href="https://arxiv.org/abs/2405.15939">[arXiv]</a>
    </p>
    <p style="margin: 0; font-size: 0.8em;">
      We introduce SynPoseDiv, a novel framework that enhances synthetic aerial-view datasets by generating realistic and diverse 3D human poses using diffusion models and image translation, leading to significantly improved detection accuracy, especially in low-shot scenarios.
    </p>
  </div>
  <div class="enlarged-image-container">
    <img src="/images/synposediv.jpg" alt="SynPoseDiv Enlarged" class="enlarged-image">
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 20px; position: relative;">
  <img src="/images/synanalysis.jpg" alt="SynAnalysis" style="width: 150px; height: auto; margin-right: 20px; border-radius: 4px;" class="original-image">
  <div>
    <h3 style="margin: 0; font-size: 0.8em;">
      Exploring the Impact of Synthetic Data for Aerial-view Human Detection
    </h3>
    <p style="margin: 5px 0; font-size: 0.8em;">
      Hyungtae Lee, Yan Zhang, <strong>Yi-Ting Shen</strong>, Heesung Kwon, and Shuvra S. Bhattacharyya
      <br>
      <a href="https://arxiv.org/abs/2405.15203">[arXiv]</a>
    </p>
    <p style="margin: 0; font-size: 0.8em;">
      We investigate key factors influencing sim-to-real transfer in UAV-based human recognition and reveal how strategic selection of synthetic data can significantly improve model performance and domain generalization, offering insights that challenge common misconceptions about synthetic data usage.
    </p>
  </div>
  <div class="enlarged-image-container">
    <img src="/images/synanalysis.jpg" alt="SynAnalysis Enlarged" class="enlarged-image">
  </div>
</div>

Publications
======

<div style="display: flex; align-items: flex-start; margin-bottom: 20px; position: relative;">
  <img src="/images/egofall.jpg" alt="EgoFall" style="width: 150px; height: auto; margin-right: 20px; border-radius: 4px;" class="original-image">
  <div>
    <h3 style="margin: 0; font-size: 0.8em;">
      Real-Time Privacy-Preserving Fall Risk Assessment with a Single Body-Worn Tracking Camera
    </h3>
    <p style="margin: 5px 0; font-size: 0.8em;">
      Chiao-Yi Wang, Faranguisse Kakhi Sadrieh, <strong>Yi-Ting Shen</strong>, Giovanni Oppizzi, Li-Qun Zhang, and Yang Tao
      <br>
      <em>ICASSP 2024</em>
      <br>
      <a href="https://ieeexplore.ieee.org/document/10447770">[Paper]</a>
    </p>
    <p style="margin: 0; font-size: 0.8em;">
      We propose EgoFall, a real-time, privacy-preserving fall risk assessment system that uses a chest-mounted camera and a lightweight CNN-Transformer model to analyze ego-body motion, enabling personalized fall prevention without relying on multiple wearable sensors.
    </p>
  </div>
  <div class="enlarged-image-container">
    <img src="/images/egofall.jpg" alt="EgoFall Enlarged" class="enlarged-image">
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 20px; position: relative;">
  <img src="/images/memo.jpg" alt="MEMO" style="width: 150px; height: auto; margin-right: 20px; border-radius: 4px;" class="original-image">
  <div>
    <h3 style="margin: 0; font-size: 0.8em;">
      MEMO: Dataset and Methods for Robust Multimodal Retinal Image Registration with Large or Small Vessel Density Differences
    </h3>
    <p style="margin: 5px 0; font-size: 0.8em;">
      Chiao-Yi Wang, Faranguisse Kakhi Sadrieh, <strong>Yi-Ting Shen</strong>, Shih-En Chen, Sarah Kim, Victoria Chen, Achyut Raghavendra, Dongyi Wang, Osamah Saeedi, and Yang Tao
      <br>
      <em>Biomedical Optics Express 2024</em>
      <br>
      <a href="https://opg.optica.org/boe/fulltext.cfm?uri=boe-15-5-3457&id=549452">[Paper]</a> <a href="https://chiaoyiwang0424.github.io/MEMO/">[Dataset]</a>
    </p>
    <p style="margin: 0; font-size: 0.8em;">
      We introduce MEMO, the first public multimodal EMA-OCTA retinal image dataset, and propose VDD-Reg, a deep learning framework that enables robust retinal image registration across large vessel density differences, advancing accurate capillary blood flow measurement for ocular disease diagnosis.
    </p>
  </div>
  <div class="enlarged-image-container">
    <img src="/images/memo.jpg" alt="MEMO Enlarged" class="enlarged-image">
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 20px; position: relative;">
  <img src="/images/shellcollect.jpg" alt="ShellCollect" style="width: 150px; height: auto; margin-right: 20px; border-radius: 4px;" class="original-image">
  <div>
    <h3 style="margin: 0; font-size: 0.8em;">
      ShellCollect: A Framework for Smart Precision Shellfish Harvesting Using Data Collection Path Planning
    </h3>
    <p style="margin: 5px 0; font-size: 0.8em;">
      Chiao-Yi Wang, ADP Guru Nandhan, <strong>Yi-Ting Shen</strong>, Wei-Yu Chen, Sandip Sharan Senthil Kumar, Alexander Long, Alan Williams, Gudjon Magnusson, Allen Pattillo, Don Webster, Matthew Gray, Miao Yu, and Yang Tao
      <br>
      <em>IEEE Access 2024</em>
      <br>
      <a href="https://ieeexplore.ieee.org/document/10766580">[Paper]</a>
    </p>
    <p style="margin: 0; font-size: 0.8em;">
      We present ShellCollect, the first smart precision shellfish harvesting framework that plans efficient dredging paths based on underwater oyster distributions, significantly improving harvesting efficiency, and validate its effectiveness through simulation and real-world field tests.
    </p>
  </div>
  <div class="enlarged-image-container">
    <img src="/images/shellcollect.jpg" alt="ShellCollect Enlarged" class="enlarged-image">
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 20px; position: relative;">
  <img src="/images/ptl.png" alt="PTL" style="width: 150px; height: auto; margin-right: 20px; border-radius: 4px;" class="original-image">
  <div>
    <h3 style="margin: 0; font-size: 0.8em;">
      Progressive Transformation Learning for Leveraging Virtual Images in Training
    </h3>
    <p style="margin: 5px 0; font-size: 0.8em;">
      <strong>Yi-Ting Shen*</strong>, Hyungtae Lee*, Heesung Kwon, and Shuvra S. Bhattacharyya (* equal contribution)
      <br>
      <em>CVPR 2023</em>
      <br>
      <strong style="color: red;">Selected as a Highlight among the top 10% of accepted papers</strong>
      <br>
      <a href="https://arxiv.org/abs/2211.01778">[arxiv]</a> <a href="https://gitlab.umiacs.umd.edu/dspcad/ptl-release">[Code]</a> <a href="https://www.youtube.com/watch?v=-P1pyGn-1zw&ab_channel=Yi-TingShen">[Video]</a>
    </p>
    <p style="margin: 0; font-size: 0.8em;">
      We introduce Progressive Transformation Learning (PTL), a novel framework that progressively transforms and selects virtual UAV images based on domain gap measurements to enhance realism and improve object detection performance, particularly in low-data and cross-domain scenarios.
    </p>
  </div>
  <div class="enlarged-image-container">
    <img src="/images/ptl.png" alt="PTL Enlarged" class="enlarged-image">
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 20px; position: relative;">
  <img src="/images/archangel.png" alt="Archangel" style="width: 150px; height: auto; margin-right: 20px; border-radius: 4px;" class="original-image">
  <div>
    <h3 style="margin: 0; font-size: 0.8em;">
      Archangel: A Hybrid UAV-based Human Detection Benchmark with Position and Pose Metadata
    </h3>
    <p style="margin: 5px 0; font-size: 0.8em;">
      <strong>Yi-Ting Shen</strong>, Yaesop Lee, Heesung Kwon, Damon M Conover, Shuvra S. Bhattacharyya, Nikolas Vale, Joshua D Gray, G Jeremy Leong, Kenneth Evensen, and Frank Skirlo
      <br>
      <em>IEEE Access 2023</em>
      <br>
      <a href="https://arxiv.org/abs/2209.00128">[arxiv]</a> <a href="https://ieeexplore.ieee.org/abstract/document/10196325">[Paper]</a> <a href="https://a2i2-archangel.vision/">[Dataset]</a>
    </p>
    <p style="margin: 0; font-size: 0.8em;">
      We introduce Archangel, the first UAV-based object detection dataset combining real and synthetic data with detailed metadata on UAV position and object pose, enabling precise model diagnosis and providing new insights into learning robust, variation-invariant detection models.
    </p>
  </div>
  <div class="enlarged-image-container">
    <img src="/images/archangel.png" alt="Archangel Enlarged" class="enlarged-image">
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 20px; position: relative;">
  <img src="/images/ddhc.png" alt="DDHC" style="width: 150px; height: auto; margin-right: 20px; border-radius: 4px;" class="original-image">
  <div>
    <h3 style="margin: 0; font-size: 0.8em;">
      DCT-based Hyperspectral Image Classification on Resource-Constrained Platforms
    </h3>
    <p style="margin: 5px 0; font-size: 0.8em;">
      Eung-Joo Lee, <strong>Yi-Ting Shen</strong>, Lei Pan, Zhu Li, and Shuvra S. Bhattacharyya
      <br>
      <em>WHISPERS 2021</em>
      <br>
      <a href="https://ieeexplore.ieee.org/document/9483973">[Paper]</a>
    </p>
    <p style="margin: 0; font-size: 0.8em;">
      We propose a flexible deep learning framework for hyperspectral image classification that learns from discrete cosine transform (DCT) coefficients, enabling efficient accuracy–computation trade-offs and streamlined deployment on resource-constrained platforms.
    </p>
  </div>
  <div class="enlarged-image-container">
    <img src="/images/ddhc.png" alt="DDHC Enlarged" class="enlarged-image">
  </div>
</div>

<div style="display: flex; align-items: flex-start; margin-bottom: 20px; position: relative;">
  <img src="/images/depthweak.png" alt="DepthWeak" style="width: 150px; height: auto; margin-right: 20px; border-radius: 4px;" class="original-image">
  <div>
    <h3 style="margin: 0; font-size: 0.8em;">
      What Synthesis is Missing: Depth Adaptation Integrated with Weak Supervision for Indoor Scene Parsing
    </h3>
    <p style="margin: 5px 0; font-size: 0.8em;">
      Keng-Chi Liu, <strong>Yi-Ting Shen</strong>, Jan P Klopp, and Liang-Gee Chen
      <br>
      <em>ICCV 2019</em>
      <br>
      <a href="https://arxiv.org/abs/1903.09781">[arxiv]</a>
    </p>
    <p style="margin: 0; font-size: 0.8em;">
      We propose a novel teacher-student framework for weakly supervised scene parsing that combines synthetic depth-based domain transfer with image-level weak labels via a contour-based integration scheme, significantly narrowing the performance gap to fully supervised methods while reducing annotation effort.
    </p>
  </div>
  <div class="enlarged-image-container">
    <img src="/images/depthweak.png" alt="DepthWeak Enlarged" class="enlarged-image">
  </div>
</div>

<style>
/* Add this CSS to your page or a linked stylesheet */
.original-image {
  z-index: 1;
}

.enlarged-image-container {
  display: none;
  position: absolute;
  top: 0;
  left: 0; /* Adjust to position beside the original image */
  z-index: 10; /* Ensure it appears above other elements */
}

.enlarged-image {
  width: 600px; /* Adjust size for enlargement */
  height: auto;
  border: 2px solid #ccc; /* Optional: Add a border for better visibility */
  background: white; /* Optional: Add a background to avoid overlap issues */
}

.original-image:hover ~ .enlarged-image-container {
  display: block;
}
</style>

Site-wide configuration
------
The main configuration file for the site is in the base directory in [_config.yml](https://github.com/academicpages/academicpages.github.io/blob/master/_config.yml), which defines the content in the sidebars and other site-wide features. You will need to replace the default variables with ones about yourself and your site's github repository. The configuration file for the top menu is in [_data/navigation.yml](https://github.com/academicpages/academicpages.github.io/blob/master/_data/navigation.yml). For example, if you don't have a portfolio or blog posts, you can remove those items from that navigation.yml file to remove them from the header. 

Create content & metadata
------
For site content, there is one markdown file for each type of content, which are stored in directories like _publications, _talks, _posts, _teaching, or _pages. For example, each talk is a markdown file in the [_talks directory](https://github.com/academicpages/academicpages.github.io/tree/master/_talks). At the top of each markdown file is structured data in YAML about the talk, which the theme will parse to do lots of cool stuff. The same structured data about a talk is used to generate the list of talks on the [Talks page](https://academicpages.github.io/talks), each [individual page](https://academicpages.github.io/talks/2012-03-01-talk-1) for specific talks, the talks section for the [CV page](https://academicpages.github.io/cv), and the [map of places you've given a talk](https://academicpages.github.io/talkmap.html) (if you run this [python file](https://github.com/academicpages/academicpages.github.io/blob/master/talkmap.py) or [Jupyter notebook](https://github.com/academicpages/academicpages.github.io/blob/master/talkmap.ipynb), which creates the HTML for the map based on the contents of the _talks directory).

**Markdown generator**

The repository includes [a set of Jupyter notebooks](https://github.com/academicpages/academicpages.github.io/tree/master/markdown_generator
) that converts a CSV containing structured data about talks or presentations into individual markdown files that will be properly formatted for the Academic Pages template. The sample CSVs in that directory are the ones I used to create my own personal website at stuartgeiger.com. My usual workflow is that I keep a spreadsheet of my publications and talks, then run the code in these notebooks to generate the markdown files, then commit and push them to the GitHub repository.

How to edit your site's GitHub repository
------
Many people use a git client to create files on their local computer and then push them to GitHub's servers. If you are not familiar with git, you can directly edit these configuration and markdown files directly in the github.com interface. Navigate to a file (like [this one](https://github.com/academicpages/academicpages.github.io/blob/master/_talks/2012-03-01-talk-1.md) and click the pencil icon in the top right of the content preview (to the right of the "Raw | Blame | History" buttons). You can delete a file by clicking the trashcan icon to the right of the pencil icon. You can also create new files or upload files by navigating to a directory and clicking the "Create new file" or "Upload files" buttons. 

Example: editing a markdown file for a talk
![Editing a markdown file for a talk](/images/editing-talk.png)

For more info
------
More info about configuring Academic Pages can be found in [the guide](https://academicpages.github.io/markdown/), the [growing wiki](https://github.com/academicpages/academicpages.github.io/wiki), and you can always [ask a question on GitHub](https://github.com/academicpages/academicpages.github.io/discussions). The [guides for the Minimal Mistakes theme](https://mmistakes.github.io/minimal-mistakes/docs/configuration/) (which this theme was forked from) might also be helpful.
